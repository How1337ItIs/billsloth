#!/usr/bin/env python3
"""
Bill Sloth Business Analytics Service
Comprehensive analytics for VRBO properties and business partnerships
"""

import asyncio
import os
import json
from datetime import datetime, timedelta, date
from typing import List, Optional, Dict, Any
from decimal import Decimal
from enum import Enum

import uvicorn
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
import pandas as pd
import numpy as np
from loguru import logger
import redis.asyncio as redis
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import text
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64

# Configure logging
logger.add(
    "/app/logs/analytics.log",
    rotation="1 day",
    retention="30 days",
    level="INFO"
)

app = FastAPI(
    title="Bill Sloth Business Analytics",
    version="1.0.0",
    description="Business intelligence and analytics for vacation rentals and partnerships"
)

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Database connection
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://bill:sloth_secure_2025@localhost:5432/bill_business")
engine = create_async_engine(DATABASE_URL)
AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

# Redis connection for caching
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
redis_client = redis.from_url(REDIS_URL)

# Set plotting style
sns.set_theme(style="whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

class TimeRange(str, Enum):
    LAST_7_DAYS = "last_7_days"
    LAST_30_DAYS = "last_30_days"
    LAST_90_DAYS = "last_90_days"
    YEAR_TO_DATE = "year_to_date"
    LAST_YEAR = "last_year"
    CUSTOM = "custom"

class MetricType(str, Enum):
    REVENUE = "revenue"
    OCCUPANCY = "occupancy"
    ADR = "adr"  # Average Daily Rate
    REVPAR = "revpar"  # Revenue per Available Room
    BOOKINGS = "bookings"
    GUEST_SATISFACTION = "guest_satisfaction"

class RevenueBreakdown(BaseModel):
    property_id: str
    property_name: str
    total_revenue: Decimal
    room_revenue: Decimal
    cleaning_fees: Decimal
    other_fees: Decimal
    occupancy_rate: float
    adr: float
    revpar: float

class PropertyPerformance(BaseModel):
    property_id: str
    property_name: str
    metrics: Dict[str, Any]
    period: str
    comparison_to_previous: Dict[str, float]

class PartnershipAnalytics(BaseModel):
    partner_name: str
    total_deals: int
    total_revenue: Decimal
    average_deal_size: Decimal
    conversion_rate: float
    roi: float

@app.on_startup
async def startup_event():
    """Initialize services on startup"""
    logger.info("üöÄ Starting Bill Sloth Business Analytics Service")
    
    # Test database connection
    try:
        async with AsyncSessionLocal() as session:
            await session.execute("SELECT 1")
        logger.info("‚úÖ Database connection successful")
    except Exception as e:
        logger.error(f"‚ùå Database connection failed: {e}")
    
    # Test Redis connection
    try:
        await redis_client.ping()
        logger.info("‚úÖ Redis connection successful")
    except Exception as e:
        logger.error(f"‚ùå Redis connection failed: {e}")
    
    # Start background tasks
    asyncio.create_task(cache_refresher())
    asyncio.create_task(analytics_aggregator())

@app.get("/")
async def root():
    return {
        "message": "Bill Sloth Business Analytics Service",
        "status": "Analyzing your business performance!",
        "features": [
            "Revenue analytics",
            "Occupancy tracking",
            "Partnership ROI",
            "Predictive insights"
        ]
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "business-analytics",
        "version": "1.0.0"
    }

@app.get("/dashboard/overview")
async def dashboard_overview(time_range: TimeRange = TimeRange.LAST_30_DAYS):
    """Get dashboard overview metrics"""
    try:
        # Check cache first
        cache_key = f"dashboard:overview:{time_range.value}"
        cached_data = await redis_client.get(cache_key)
        
        if cached_data:
            return json.loads(cached_data)
        
        # Calculate date range
        start_date, end_date = get_date_range(time_range)
        
        # Fetch metrics
        metrics = await calculate_overview_metrics(start_date, end_date)
        
        # Cache for 1 hour
        await redis_client.setex(cache_key, 3600, json.dumps(metrics, default=str))
        
        return metrics
        
    except Exception as e:
        logger.error(f"Error generating dashboard overview: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/properties/performance")
async def properties_performance(
    time_range: TimeRange = TimeRange.LAST_30_DAYS,
    property_ids: Optional[List[str]] = Query(None)
):
    """Get property performance metrics"""
    try:
        start_date, end_date = get_date_range(time_range)
        
        # Get property performance data
        performance_data = await get_property_performance(
            start_date, end_date, property_ids
        )
        
        return {"properties": performance_data, "period": time_range.value}
        
    except Exception as e:
        logger.error(f"Error fetching property performance: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/revenue/breakdown")
async def revenue_breakdown(
    time_range: TimeRange = TimeRange.LAST_30_DAYS,
    group_by: str = "property"
):
    """Get detailed revenue breakdown"""
    try:
        start_date, end_date = get_date_range(time_range)
        
        breakdown = await calculate_revenue_breakdown(
            start_date, end_date, group_by
        )
        
        return {
            "breakdown": breakdown,
            "period": time_range.value,
            "grouped_by": group_by
        }
        
    except Exception as e:
        logger.error(f"Error calculating revenue breakdown: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/occupancy/trends")
async def occupancy_trends(
    time_range: TimeRange = TimeRange.LAST_90_DAYS,
    property_id: Optional[str] = None
):
    """Get occupancy trends over time"""
    try:
        start_date, end_date = get_date_range(time_range)
        
        trends = await calculate_occupancy_trends(
            start_date, end_date, property_id
        )
        
        return {
            "trends": trends,
            "period": time_range.value,
            "property_id": property_id
        }
        
    except Exception as e:
        logger.error(f"Error calculating occupancy trends: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/partnerships/analytics")
async def partnerships_analytics(
    time_range: TimeRange = TimeRange.LAST_30_DAYS
):
    """Get partnership performance analytics"""
    try:
        start_date, end_date = get_date_range(time_range)
        
        analytics = await calculate_partnership_analytics(start_date, end_date)
        
        return {
            "partnerships": analytics,
            "period": time_range.value,
            "total_partnerships": len(analytics)
        }
        
    except Exception as e:
        logger.error(f"Error calculating partnership analytics: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/predictions/revenue")
async def revenue_predictions(months_ahead: int = 3):
    """Get revenue predictions for future months"""
    try:
        predictions = await predict_revenue(months_ahead)
        
        return {
            "predictions": predictions,
            "months_ahead": months_ahead,
            "confidence_level": 0.85
        }
        
    except Exception as e:
        logger.error(f"Error generating revenue predictions: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/reports/generate/{report_type}")
async def generate_report(
    report_type: str,
    time_range: TimeRange = TimeRange.LAST_30_DAYS,
    format: str = "pdf"
):
    """Generate detailed business reports"""
    try:
        start_date, end_date = get_date_range(time_range)
        
        if report_type == "monthly_summary":
            report_data = await generate_monthly_summary(start_date, end_date)
        elif report_type == "property_performance":
            report_data = await generate_property_report(start_date, end_date)
        elif report_type == "tax_summary":
            report_data = await generate_tax_summary(start_date, end_date)
        else:
            raise HTTPException(status_code=400, detail="Invalid report type")
        
        # Format report based on requested format
        if format == "json":
            return report_data
        elif format == "csv":
            return StreamingResponse(
                generate_csv_report(report_data),
                media_type="text/csv",
                headers={"Content-Disposition": f"attachment; filename={report_type}_{datetime.now().strftime('%Y%m%d')}.csv"}
            )
        else:
            # PDF generation would go here
            raise HTTPException(status_code=501, detail="PDF generation not implemented")
            
    except Exception as e:
        logger.error(f"Error generating report: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/charts/{chart_type}")
async def generate_chart(
    chart_type: str,
    time_range: TimeRange = TimeRange.LAST_30_DAYS,
    property_id: Optional[str] = None
):
    """Generate analytics charts"""
    try:
        start_date, end_date = get_date_range(time_range)
        
        if chart_type == "revenue_trend":
            chart = await create_revenue_trend_chart(start_date, end_date, property_id)
        elif chart_type == "occupancy_heatmap":
            chart = await create_occupancy_heatmap(start_date, end_date, property_id)
        elif chart_type == "booking_sources":
            chart = await create_booking_sources_chart(start_date, end_date)
        elif chart_type == "partnership_roi":
            chart = await create_partnership_roi_chart(start_date, end_date)
        else:
            raise HTTPException(status_code=400, detail="Invalid chart type")
        
        return {"chart": chart, "type": chart_type}
        
    except Exception as e:
        logger.error(f"Error generating chart: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def calculate_overview_metrics(start_date: date, end_date: date) -> Dict:
    """Calculate overview metrics for dashboard"""
    async with AsyncSessionLocal() as session:
        # Total revenue
        revenue_query = """
        SELECT 
            COALESCE(SUM(total_amount), 0) as total_revenue,
            COUNT(DISTINCT property_id) as active_properties,
            COUNT(*) as total_bookings,
            AVG(nights) as avg_stay_length
        FROM bookings
        WHERE check_in BETWEEN :start_date AND :end_date
        AND booking_status = 'confirmed'
        """
        
        revenue_result = await session.execute(
            text(revenue_query),
            {"start_date": start_date, "end_date": end_date}
        )
        revenue_data = revenue_result.first()
        
        # Calculate occupancy rate
        occupancy_query = """
        SELECT 
            SUM(nights) as booked_nights,
            COUNT(DISTINCT property_id) * :total_days as available_nights
        FROM bookings
        WHERE check_in BETWEEN :start_date AND :end_date
        AND booking_status = 'confirmed'
        """
        
        total_days = (end_date - start_date).days
        occupancy_result = await session.execute(
            text(occupancy_query),
            {"start_date": start_date, "end_date": end_date, "total_days": total_days}
        )
        occupancy_data = occupancy_result.first()
        
        occupancy_rate = 0
        if occupancy_data.available_nights > 0:
            occupancy_rate = (occupancy_data.booked_nights / occupancy_data.available_nights) * 100
        
        # Partnership revenue
        partnership_query = """
        SELECT 
            COALESCE(SUM(commission_amount), 0) as partnership_revenue,
            COUNT(DISTINCT partner) as active_partners
        FROM commissions
        WHERE date BETWEEN :start_date AND :end_date
        """
        
        partnership_result = await session.execute(
            text(partnership_query),
            {"start_date": start_date, "end_date": end_date}
        )
        partnership_data = partnership_result.first()
        
        # Guest satisfaction (from reviews)
        satisfaction_query = """
        SELECT AVG(rating) as avg_rating
        FROM reviews
        WHERE review_date BETWEEN :start_date AND :end_date
        """
        
        satisfaction_result = await session.execute(
            text(satisfaction_query),
            {"start_date": start_date, "end_date": end_date}
        )
        satisfaction_data = satisfaction_result.first()
        
        return {
            "revenue": {
                "total": float(revenue_data.total_revenue or 0),
                "vrbo": float(revenue_data.total_revenue or 0) * 0.8,  # Estimate
                "partnerships": float(partnership_data.partnership_revenue or 0),
                "other": float(revenue_data.total_revenue or 0) * 0.2  # Estimate
            },
            "properties": {
                "active": revenue_data.active_properties or 0,
                "occupancy_rate": round(occupancy_rate, 2),
                "avg_daily_rate": float(revenue_data.total_revenue or 0) / (occupancy_data.booked_nights or 1)
            },
            "bookings": {
                "total": revenue_data.total_bookings or 0,
                "avg_stay_length": float(revenue_data.avg_stay_length or 0),
                "avg_booking_value": float(revenue_data.total_revenue or 0) / (revenue_data.total_bookings or 1)
            },
            "partnerships": {
                "active": partnership_data.active_partners or 0,
                "revenue": float(partnership_data.partnership_revenue or 0)
            },
            "guest_satisfaction": {
                "avg_rating": float(satisfaction_data.avg_rating or 0),
                "total_reviews": 0  # Would need another query
            },
            "period": {
                "start": start_date.isoformat(),
                "end": end_date.isoformat(),
                "days": total_days
            }
        }

async def get_property_performance(
    start_date: date,
    end_date: date,
    property_ids: Optional[List[str]] = None
) -> List[Dict]:
    """Get detailed property performance metrics"""
    async with AsyncSessionLocal() as session:
        base_query = """
        SELECT 
            p.vrbo_property_id,
            p.name,
            COUNT(b.id) as total_bookings,
            COALESCE(SUM(b.total_amount), 0) as total_revenue,
            COALESCE(SUM(b.nights), 0) as total_nights,
            AVG(b.total_amount) as avg_booking_value,
            AVG(b.nights) as avg_stay_length,
            MAX(b.check_in) as last_booking
        FROM properties p
        LEFT JOIN bookings b ON p.id = b.property_id
            AND b.check_in BETWEEN :start_date AND :end_date
            AND b.booking_status = 'confirmed'
        WHERE p.sync_status = 'active'
        """
        
        params = {"start_date": start_date, "end_date": end_date}
        
        if property_ids:
            base_query += " AND p.vrbo_property_id IN :property_ids"
            params["property_ids"] = tuple(property_ids)
        
        base_query += " GROUP BY p.vrbo_property_id, p.name"
        
        result = await session.execute(text(base_query), params)
        
        performance_data = []
        total_days = (end_date - start_date).days
        
        for row in result:
            occupancy_rate = (row.total_nights / total_days) * 100 if total_days > 0 else 0
            adr = float(row.total_revenue) / row.total_nights if row.total_nights > 0 else 0
            revpar = float(row.total_revenue) / total_days if total_days > 0 else 0
            
            performance_data.append({
                "property_id": row.vrbo_property_id,
                "property_name": row.name,
                "metrics": {
                    "total_bookings": row.total_bookings,
                    "total_revenue": float(row.total_revenue),
                    "occupancy_rate": round(occupancy_rate, 2),
                    "adr": round(adr, 2),
                    "revpar": round(revpar, 2),
                    "avg_booking_value": float(row.avg_booking_value or 0),
                    "avg_stay_length": float(row.avg_stay_length or 0),
                    "last_booking": row.last_booking.isoformat() if row.last_booking else None
                }
            })
        
        return performance_data

async def calculate_revenue_breakdown(
    start_date: date,
    end_date: date,
    group_by: str
) -> List[Dict]:
    """Calculate detailed revenue breakdown"""
    async with AsyncSessionLocal() as session:
        if group_by == "property":
            query = """
            SELECT 
                p.name as group_name,
                SUM(b.total_amount) as total_revenue,
                SUM(b.total_amount * 0.85) as room_revenue,
                SUM(b.total_amount * 0.15) as fees_revenue,
                COUNT(b.id) as booking_count
            FROM bookings b
            JOIN properties p ON b.property_id = p.id
            WHERE b.check_in BETWEEN :start_date AND :end_date
            AND b.booking_status = 'confirmed'
            GROUP BY p.name
            ORDER BY total_revenue DESC
            """
        elif group_by == "month":
            query = """
            SELECT 
                TO_CHAR(check_in, 'YYYY-MM') as group_name,
                SUM(total_amount) as total_revenue,
                SUM(total_amount * 0.85) as room_revenue,
                SUM(total_amount * 0.15) as fees_revenue,
                COUNT(id) as booking_count
            FROM bookings
            WHERE check_in BETWEEN :start_date AND :end_date
            AND booking_status = 'confirmed'
            GROUP BY TO_CHAR(check_in, 'YYYY-MM')
            ORDER BY group_name
            """
        else:
            raise ValueError(f"Invalid group_by value: {group_by}")
        
        result = await session.execute(
            text(query),
            {"start_date": start_date, "end_date": end_date}
        )
        
        breakdown = []
        for row in result:
            breakdown.append({
                "group": row.group_name,
                "total_revenue": float(row.total_revenue),
                "room_revenue": float(row.room_revenue),
                "fees_revenue": float(row.fees_revenue),
                "booking_count": row.booking_count,
                "avg_booking_value": float(row.total_revenue) / row.booking_count if row.booking_count > 0 else 0
            })
        
        return breakdown

async def calculate_occupancy_trends(
    start_date: date,
    end_date: date,
    property_id: Optional[str] = None
) -> List[Dict]:
    """Calculate occupancy trends over time"""
    async with AsyncSessionLocal() as session:
        # Generate date series
        date_series_query = """
        SELECT generate_series(
            :start_date::date,
            :end_date::date,
            '1 day'::interval
        )::date as date
        """
        
        # Get booked nights per date
        if property_id:
            bookings_query = """
            SELECT 
                d.date,
                COUNT(DISTINCT b.id) as bookings,
                CASE WHEN COUNT(b.id) > 0 THEN 100 ELSE 0 END as occupancy
            FROM (
                SELECT generate_series(:start_date::date, :end_date::date, '1 day'::interval)::date as date
            ) d
            LEFT JOIN bookings b ON d.date BETWEEN b.check_in AND b.check_out - INTERVAL '1 day'
                AND b.booking_status = 'confirmed'
                AND b.property_id = (SELECT id FROM properties WHERE vrbo_property_id = :property_id)
            GROUP BY d.date
            ORDER BY d.date
            """
            params = {"start_date": start_date, "end_date": end_date, "property_id": property_id}
        else:
            bookings_query = """
            SELECT 
                d.date,
                COUNT(DISTINCT b.id) as bookings,
                (COUNT(DISTINCT b.id)::float / COUNT(DISTINCT p.id)) * 100 as occupancy
            FROM (
                SELECT generate_series(:start_date::date, :end_date::date, '1 day'::interval)::date as date
            ) d
            CROSS JOIN properties p
            LEFT JOIN bookings b ON d.date BETWEEN b.check_in AND b.check_out - INTERVAL '1 day'
                AND b.booking_status = 'confirmed'
                AND b.property_id = p.id
            WHERE p.sync_status = 'active'
            GROUP BY d.date
            ORDER BY d.date
            """
            params = {"start_date": start_date, "end_date": end_date}
        
        result = await session.execute(text(bookings_query), params)
        
        trends = []
        for row in result:
            trends.append({
                "date": row.date.isoformat(),
                "bookings": row.bookings,
                "occupancy_rate": float(row.occupancy)
            })
        
        return trends

async def calculate_partnership_analytics(
    start_date: date,
    end_date: date
) -> List[Dict]:
    """Calculate partnership performance analytics"""
    async with AsyncSessionLocal() as session:
        query = """
        SELECT 
            partner_name,
            COUNT(DISTINCT deal_id) as total_deals,
            SUM(deal_value) as total_revenue,
            AVG(deal_value) as avg_deal_size,
            SUM(commission_amount) as total_commission,
            SUM(CASE WHEN status = 'won' THEN 1 ELSE 0 END)::float / COUNT(*) as conversion_rate
        FROM partnerships
        WHERE date BETWEEN :start_date AND :end_date
        GROUP BY partner_name
        ORDER BY total_revenue DESC
        """
        
        result = await session.execute(
            text(query),
            {"start_date": start_date, "end_date": end_date}
        )
        
        analytics = []
        for row in result:
            roi = (float(row.total_commission) / float(row.total_revenue) * 100) if row.total_revenue > 0 else 0
            
            analytics.append({
                "partner_name": row.partner_name,
                "total_deals": row.total_deals,
                "total_revenue": float(row.total_revenue),
                "avg_deal_size": float(row.avg_deal_size),
                "total_commission": float(row.total_commission),
                "conversion_rate": float(row.conversion_rate) * 100,
                "roi": round(roi, 2)
            })
        
        return analytics

async def predict_revenue(months_ahead: int) -> List[Dict]:
    """Simple revenue prediction based on historical trends"""
    async with AsyncSessionLocal() as session:
        # Get historical monthly revenue
        query = """
        SELECT 
            TO_CHAR(check_in, 'YYYY-MM') as month,
            SUM(total_amount) as revenue
        FROM bookings
        WHERE check_in >= CURRENT_DATE - INTERVAL '12 months'
        AND booking_status = 'confirmed'
        GROUP BY TO_CHAR(check_in, 'YYYY-MM')
        ORDER BY month
        """
        
        result = await session.execute(text(query))
        
        # Convert to pandas for analysis
        data = []
        for row in result:
            data.append({
                "month": row.month,
                "revenue": float(row.revenue)
            })
        
        if not data:
            return []
        
        df = pd.DataFrame(data)
        
        # Simple linear regression trend
        df['month_num'] = range(len(df))
        z = np.polyfit(df['month_num'], df['revenue'], 1)
        p = np.poly1d(z)
        
        # Generate predictions
        predictions = []
        last_month_num = len(df)
        
        for i in range(1, months_ahead + 1):
            future_month = datetime.now() + timedelta(days=30*i)
            predicted_revenue = p(last_month_num + i)
            
            # Add some randomness for realistic variance
            variance = predicted_revenue * 0.1
            min_revenue = predicted_revenue - variance
            max_revenue = predicted_revenue + variance
            
            predictions.append({
                "month": future_month.strftime('%Y-%m'),
                "predicted_revenue": round(predicted_revenue, 2),
                "min_revenue": round(min_revenue, 2),
                "max_revenue": round(max_revenue, 2),
                "confidence": 0.85
            })
        
        return predictions

async def create_revenue_trend_chart(
    start_date: date,
    end_date: date,
    property_id: Optional[str] = None
) -> str:
    """Create revenue trend chart and return as base64"""
    async with AsyncSessionLocal() as session:
        if property_id:
            query = """
            SELECT 
                DATE_TRUNC('week', check_in) as week,
                SUM(total_amount) as revenue
            FROM bookings b
            JOIN properties p ON b.property_id = p.id
            WHERE b.check_in BETWEEN :start_date AND :end_date
            AND b.booking_status = 'confirmed'
            AND p.vrbo_property_id = :property_id
            GROUP BY week
            ORDER BY week
            """
            params = {"start_date": start_date, "end_date": end_date, "property_id": property_id}
        else:
            query = """
            SELECT 
                DATE_TRUNC('week', check_in) as week,
                SUM(total_amount) as revenue
            FROM bookings
            WHERE check_in BETWEEN :start_date AND :end_date
            AND booking_status = 'confirmed'
            GROUP BY week
            ORDER BY week
            """
            params = {"start_date": start_date, "end_date": end_date}
        
        result = await session.execute(text(query), params)
        
        # Prepare data
        weeks = []
        revenues = []
        for row in result:
            weeks.append(row.week)
            revenues.append(float(row.revenue))
        
        # Create chart
        plt.figure(figsize=(12, 6))
        plt.plot(weeks, revenues, marker='o', linewidth=2, markersize=8)
        plt.title('Revenue Trend', fontsize=16)
        plt.xlabel('Week', fontsize=12)
        plt.ylabel('Revenue ($)', fontsize=12)
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # Convert to base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png')
        buffer.seek(0)
        chart_base64 = base64.b64encode(buffer.read()).decode()
        plt.close()
        
        return chart_base64

def get_date_range(time_range: TimeRange) -> tuple[date, date]:
    """Get start and end dates based on time range"""
    today = date.today()
    
    if time_range == TimeRange.LAST_7_DAYS:
        return today - timedelta(days=7), today
    elif time_range == TimeRange.LAST_30_DAYS:
        return today - timedelta(days=30), today
    elif time_range == TimeRange.LAST_90_DAYS:
        return today - timedelta(days=90), today
    elif time_range == TimeRange.YEAR_TO_DATE:
        return date(today.year, 1, 1), today
    elif time_range == TimeRange.LAST_YEAR:
        return date(today.year - 1, 1, 1), date(today.year - 1, 12, 31)
    else:
        return today - timedelta(days=30), today

async def cache_refresher():
    """Background task to refresh cached data"""
    while True:
        try:
            # Refresh dashboard cache
            for time_range in TimeRange:
                if time_range != TimeRange.CUSTOM:
                    start_date, end_date = get_date_range(time_range)
                    metrics = await calculate_overview_metrics(start_date, end_date)
                    
                    cache_key = f"dashboard:overview:{time_range.value}"
                    await redis_client.setex(
                        cache_key,
                        3600,
                        json.dumps(metrics, default=str)
                    )
            
            logger.info("‚úÖ Cache refreshed successfully")
            
            # Refresh every hour
            await asyncio.sleep(3600)
            
        except Exception as e:
            logger.error(f"Error refreshing cache: {e}")
            await asyncio.sleep(300)  # Retry in 5 minutes

async def analytics_aggregator():
    """Background task to aggregate analytics data"""
    while True:
        try:
            # Daily aggregations at 2 AM
            now = datetime.now()
            if now.hour == 2 and now.minute < 5:
                logger.info("Starting daily analytics aggregation...")
                
                # Aggregate yesterday's data
                yesterday = date.today() - timedelta(days=1)
                
                # Would implement various aggregation tasks here
                # - Daily revenue summary
                # - Occupancy calculations
                # - Guest satisfaction scores
                # - Partnership performance
                
                logger.info("‚úÖ Daily analytics aggregation completed")
            
            # Check every 5 minutes
            await asyncio.sleep(300)
            
        except Exception as e:
            logger.error(f"Error in analytics aggregator: {e}")
            await asyncio.sleep(300)

def generate_csv_report(data: Dict) -> BytesIO:
    """Generate CSV report from data"""
    output = BytesIO()
    
    # Convert data to DataFrame
    if isinstance(data, dict) and 'properties' in data:
        df = pd.DataFrame(data['properties'])
    elif isinstance(data, list):
        df = pd.DataFrame(data)
    else:
        df = pd.DataFrame([data])
    
    # Write to CSV
    df.to_csv(output, index=False)
    output.seek(0)
    
    return output

@app.get("/kpis/realtime")
async def realtime_kpis():
    """Get real-time KPIs"""
    async with AsyncSessionLocal() as session:
        # Today's bookings
        today_bookings_query = """
        SELECT COUNT(*) as count, COALESCE(SUM(total_amount), 0) as revenue
        FROM bookings
        WHERE DATE(created_date) = CURRENT_DATE
        AND booking_status = 'confirmed'
        """
        
        result = await session.execute(text(today_bookings_query))
        today_data = result.first()
        
        # Current occupancy
        occupancy_query = """
        SELECT 
            COUNT(DISTINCT b.property_id) as occupied_properties,
            COUNT(DISTINCT p.id) as total_properties
        FROM properties p
        LEFT JOIN bookings b ON p.id = b.property_id
            AND CURRENT_DATE BETWEEN b.check_in AND b.check_out - INTERVAL '1 day'
            AND b.booking_status = 'confirmed'
        WHERE p.sync_status = 'active'
        """
        
        occupancy_result = await session.execute(text(occupancy_query))
        occupancy_data = occupancy_result.first()
        
        current_occupancy = 0
        if occupancy_data.total_properties > 0:
            current_occupancy = (occupancy_data.occupied_properties / occupancy_data.total_properties) * 100
        
        return {
            "timestamp": datetime.now().isoformat(),
            "today": {
                "new_bookings": today_data.count,
                "revenue": float(today_data.revenue)
            },
            "current_occupancy": round(current_occupancy, 2),
            "properties": {
                "occupied": occupancy_data.occupied_properties,
                "total": occupancy_data.total_properties
            }
        }

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8002,
        reload=True,
        log_level="info"
    )