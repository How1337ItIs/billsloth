#!/bin/bash
# LLM_CAPABILITY: local_ok
# Bill Sloth Local AI Setup Wizard
# One-command setup for complete AI independence

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
BOLD='\033[1m'
NC='\033[0m'

# Get script directory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
LIB_DIR="$(dirname "$SCRIPT_DIR")/lib"

# Source required libraries
source "$LIB_DIR/local_llm_integration.sh"
source "$LIB_DIR/error_handling.sh"
source "$LIB_DIR/notification.sh"

show_banner() {
    # Source aesthetic functions if available
    if [ -f "$LIB_DIR/aesthetic_functions.sh" ]; then
        source "$LIB_DIR/aesthetic_functions.sh"
        show_module_banner "BILL SLOTH LOCAL AI SETUP" "Complete AI independence - No external APIs!" "ü§ñ"
    else
        echo -e "${CYAN}"
        cat << 'EOF'
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                     ü§ñ BILL SLOTH LOCAL AI SETUP ü§ñ                         ‚ïë
‚ïë                                                                              ‚ïë
‚ïë         Complete AI independence with local LLM and voice processing        ‚ïë
‚ïë              No more external dependencies or API costs!                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF
        echo -e "${NC}"
    fi
}

check_system_requirements() {
    echo -e "${BLUE}üîç Checking system requirements...${NC}"
    
    local requirements_met=true
    
    # Check available disk space (need 15GB for full setup)
    local available_gb=$(df "$HOME" | awk 'NR==2 {printf "%.1f", $4/1024/1024}')
    echo -e "   üíæ Available disk space: ${available_gb}GB"
    
    if (( $(echo "$available_gb < 15" | bc -l) )); then
        echo -e "${YELLOW}   ‚ö†Ô∏è  Warning: Less than 15GB available. Will install minimal models.${NC}"
    fi
    
    # Check RAM (recommend 8GB+)
    local total_ram_gb=$(free -g | awk 'NR==2{printf "%.1f", $2}')
    echo -e "   üß† Available RAM: ${total_ram_gb}GB"
    
    if (( $(echo "$total_ram_gb < 8" | bc -l) )); then
        echo -e "${YELLOW}   ‚ö†Ô∏è  Warning: Less than 8GB RAM. Performance may be limited.${NC}"
    fi
    
    # Check for GPU
    if command -v nvidia-smi >/dev/null 2>&1; then
        local gpu_info=$(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | head -1)
        echo -e "   üéÆ GPU detected: ${gpu_info}"
    else
        echo -e "   üéÆ No NVIDIA GPU detected - will use CPU processing"
    fi
    
    # Check internet connection
    if ping -c 1 google.com >/dev/null 2>&1; then
        echo -e "   üåê Internet connection: Available"
    else
        echo -e "${RED}   ‚ùå No internet connection - cannot download models${NC}"
        requirements_met=false
    fi
    
    echo ""
    
    if [ "$requirements_met" = true ]; then
        echo -e "${GREEN}‚úÖ System requirements check passed${NC}"
        return 0
    else
        echo -e "${RED}‚ùå System requirements not met${NC}"
        return 1
    fi
}

show_installation_options() {
    echo -e "${PURPLE}üìã Installation Options:${NC}"
    echo ""
    echo -e "${CYAN}1) üöÄ Express Setup${NC} (Recommended)"
    echo "   ‚Ä¢ Essential models (6GB download)"
    echo "   ‚Ä¢ Basic voice control"
    echo "   ‚Ä¢ Business email generation"
    echo "   ‚Ä¢ Setup time: ~15 minutes"
    echo ""
    echo -e "${CYAN}2) üèÜ Complete Setup${NC} (Power User)"
    echo "   ‚Ä¢ Full model suite (12GB download)"
    echo "   ‚Ä¢ Advanced voice processing"
    echo "   ‚Ä¢ Code assistance"
    echo "   ‚Ä¢ Business analytics"
    echo "   ‚Ä¢ Setup time: ~30 minutes"
    echo ""
    echo -e "${CYAN}3) üéØ Custom Setup${NC} (Advanced)"
    echo "   ‚Ä¢ Choose specific models"
    echo "   ‚Ä¢ Custom configurations"
    echo "   ‚Ä¢ Specialized use cases"
    echo ""
    
    while true; do
        echo -e "${BLUE}Select installation type [1-3]: ${NC}"
        read -r choice
        
        case $choice in
            1)
                echo -e "${GREEN}Selected: Express Setup${NC}"
                return 1
                ;;
            2)
                echo -e "${GREEN}Selected: Complete Setup${NC}"
                return 2
                ;;
            3)
                echo -e "${GREEN}Selected: Custom Setup${NC}"
                return 3
                ;;
            *)
                echo -e "${RED}Please enter 1, 2, or 3${NC}"
                ;;
        esac
    done
}

express_setup() {
    echo -e "${CYAN}üöÄ Starting Express Setup...${NC}"
    
    # Install Ollama if needed
    if ! check_ollama_installation; then
        echo -e "${BLUE}üì¶ Installing Ollama...${NC}"
        install_ollama
    fi
    
    # Download essential models
    echo -e "${BLUE}‚¨áÔ∏è  Downloading essential AI models...${NC}"
    echo -e "${YELLOW}   This will download ~6GB of data...${NC}"
    
    ollama pull llama3.2:3b      # General chat - 2GB
    ollama pull phi3:medium      # Business tasks - 2.5GB
    ollama pull qwen2.5:1.5b     # Lightweight assistant - 1GB
    
    # Create basic configuration
    create_llm_directories
    create_llm_config
    create_llm_integration_scripts
    
    # Start API server
    create_llm_api_wrapper
    start_llm_api_server
    
    # Test setup
    if test_local_llm "llama3.2:3b"; then
        echo -e "${GREEN}‚úÖ Express setup completed successfully!${NC}"
        show_quick_start_guide
    else
        echo -e "${RED}‚ùå Express setup failed${NC}"
        return 1
    fi
}

complete_setup() {
    echo -e "${CYAN}üèÜ Starting Complete Setup...${NC}"
    
    # Run full setup
    setup_local_llm_complete
    
    # Additional integrations
    integrate_with_voice_control
    integrate_with_business_automation
    
    # Install Whisper for voice processing
    setup_local_whisper
    
    echo -e "${GREEN}‚úÖ Complete setup finished successfully!${NC}"
    show_advanced_features_guide
}

setup_local_whisper() {
    echo -e "${BLUE}üé§ Setting up local voice processing...${NC}"
    
    # Install Whisper if not present
    if ! python3 -c "import whisper" 2>/dev/null; then
        pip3 install --user openai-whisper
    fi
    
    # Download medium model for good quality/speed balance
    python3 -c "import whisper; whisper.load_model('medium')"
    
    # Create voice processing script
    cat > "$LOCAL_LLM_DIR/voice-processor" << 'EOF'
#!/usr/bin/env python3
import whisper
import sys
import os

def process_audio(audio_file):
    """Process audio file with local Whisper model"""
    try:
        model = whisper.load_model("medium")
        result = model.transcribe(audio_file)
        return result["text"].strip()
    except Exception as e:
        return f"Error: {e}"

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: voice-processor <audio_file>")
        sys.exit(1)
    
    audio_file = sys.argv[1]
    if not os.path.exists(audio_file):
        print(f"Error: Audio file {audio_file} not found")
        sys.exit(1)
    
    text = process_audio(audio_file)
    print(text)
EOF
    
    chmod +x "$LOCAL_LLM_DIR/voice-processor"
    echo -e "${GREEN}‚úÖ Local voice processing setup completed${NC}"
}

custom_setup() {
    echo -e "${CYAN}üéØ Starting Custom Setup...${NC}"
    
    echo -e "${BLUE}Available models:${NC}"
    echo "1. llama3.2:3b (2GB) - General chat, fast"
    echo "2. phi3:medium (2.5GB) - Business tasks"
    echo "3. deepseek-coder:6.7b (4GB) - Code assistance"
    echo "4. mistral:7b (4GB) - Alternative chat model"
    echo "5. codellama:7b (4GB) - Code generation"
    echo "6. qwen2.5:1.5b (1GB) - Lightweight assistant"
    echo ""
    
    echo -e "${BLUE}Select models to install (comma-separated numbers): ${NC}"
    read -r model_choices
    
    # Install Ollama if needed
    if ! check_ollama_installation; then
        install_ollama
    fi
    
    create_llm_directories
    
    # Install selected models
    IFS=',' read -ra MODELS <<< "$model_choices"
    for model_num in "${MODELS[@]}"; do
        case ${model_num// /} in
            1) ollama pull llama3.2:3b ;;
            2) ollama pull phi3:medium ;;
            3) ollama pull deepseek-coder:6.7b ;;
            4) ollama pull mistral:7b ;;
            5) ollama pull codellama:7b ;;
            6) ollama pull qwen2.5:1.5b ;;
            *) echo -e "${YELLOW}Skipping invalid selection: $model_num${NC}" ;;
        esac
    done
    
    create_llm_config
    create_llm_integration_scripts
    create_llm_api_wrapper
    start_llm_api_server
    
    echo -e "${GREEN}‚úÖ Custom setup completed!${NC}"
}

show_quick_start_guide() {
    echo -e "${PURPLE}"
    cat << 'EOF'
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                           üöÄ QUICK START GUIDE                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF
    echo -e "${NC}"
    
    echo -e "${CYAN}üí¨ Chat with your local AI:${NC}"
    echo "   chat 'How can I improve my VRBO listing?'"
    echo ""
    
    echo -e "${CYAN}üìß Generate business emails:${NC}"
    echo "   business-assist email partnership 'Follow up on collaboration'"
    echo ""
    
    echo -e "${CYAN}üîó API Access:${NC}"
    echo "   Local API server running at: http://127.0.0.1:8899"
    echo "   OpenAI-compatible endpoints for integration"
    echo ""
    
    echo -e "${CYAN}üìä Monitor usage:${NC}"
    echo "   check-local-ai-status  # View model status and usage"
    echo ""
}

show_advanced_features_guide() {
    echo -e "${PURPLE}"
    cat << 'EOF'
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                        üèÜ ADVANCED FEATURES GUIDE                           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF
    echo -e "${NC}"
    
    echo -e "${CYAN}üé§ Voice Control Integration:${NC}"
    echo "   ‚Ä¢ Say 'Hey Bill' + your command"
    echo "   ‚Ä¢ Local voice processing (no cloud)"
    echo "   ‚Ä¢ Responds with AI-generated answers"
    echo ""
    
    echo -e "${CYAN}üíº Business Automation:${NC}"
    echo "   ‚Ä¢ Auto-generate VRBO guest emails"
    echo "   ‚Ä¢ Partnership outreach templates"
    echo "   ‚Ä¢ Revenue analysis and insights"
    echo ""
    
    echo -e "${CYAN}üë®‚Äçüíª Code Assistance:${NC}"
    echo "   code-assist 'bash script context' 'add error handling'"
    echo "   ‚Ä¢ Local code completion and review"
    echo "   ‚Ä¢ No code sent to external services"
    echo ""
    
    echo -e "${CYAN}üìà Analytics:${NC}"
    echo "   ‚Ä¢ Business data analysis with AI insights"
    echo "   ‚Ä¢ Trend prediction and recommendations"
    echo "   ‚Ä¢ Local processing for privacy"
    echo ""
}

create_status_checker() {
    cat > "$LOCAL_LLM_DIR/check-status" << 'EOF'
#!/bin/bash
# Check local AI system status

echo "ü§ñ Bill Sloth Local AI Status"
echo "================================"

# Check Ollama service
if pgrep ollama >/dev/null; then
    echo "‚úÖ Ollama service: Running"
else
    echo "‚ùå Ollama service: Not running"
fi

# Check API server
if curl -s http://127.0.0.1:8899/health >/dev/null; then
    echo "‚úÖ API server: Running"
else
    echo "‚ùå API server: Not running"
fi

# List available models
echo ""
echo "üìö Available Models:"
ollama list | tail -n +2

# Check disk usage
echo ""
echo "üíæ Storage Usage:"
du -sh ~/.ollama/models 2>/dev/null || echo "No models directory found"

# Memory usage
echo ""
echo "üß† Memory Usage:"
ps aux | grep -E "(ollama|whisper)" | grep -v grep | awk '{print $1" "$11" "$4"%"}' | head -5
EOF
    
    chmod +x "$LOCAL_LLM_DIR/check-status"
    echo "export PATH=\"\$PATH:$LOCAL_LLM_DIR\"" >> ~/.bashrc
}

main() {
    show_banner
    
    echo -e "${BLUE}Welcome to Bill Sloth Local AI Setup!${NC}"
    echo "This wizard will set up complete AI independence for your system."
    echo ""
    
    # Check system requirements
    if ! check_system_requirements; then
        echo -e "${RED}Please resolve system requirements before continuing.${NC}"
        exit 1
    fi
    
    # Show installation options
    show_installation_options
    setup_type=$?
    
    echo ""
    echo -e "${YELLOW}‚è≥ Starting installation... This may take 15-30 minutes.${NC}"
    echo -e "${YELLOW}   Feel free to grab a coffee! ‚òï${NC}"
    echo ""
    
    case $setup_type in
        1)
            express_setup
            ;;
        2)
            complete_setup
            ;;
        3)
            custom_setup
            ;;
    esac
    
    # Create status checker
    create_status_checker
    
    # Final success message
    echo ""
    echo -e "${GREEN}üéâ Local AI setup completed successfully!${NC}"
    echo -e "${CYAN}Bill Sloth now has complete AI independence!${NC}"
    echo ""
    echo -e "${PURPLE}Next steps:${NC}"
    echo "1. Run 'check-local-ai-status' to verify everything is working"
    echo "2. Try 'chat \"Hello!\"' to test your local AI"
    echo "3. Check the integration with Bill Sloth voice control"
    echo ""
    echo -e "${CYAN}Enjoy your private, local AI assistant! üöÄ${NC}"
}

# Run main function
main "$@"